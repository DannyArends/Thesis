\chapter{Introduction}
\emph{The introduction of this thesis, background information and the outline of the next chapters.}

\null
\vfill
\newpage

\section{Introduction}

Phenotype diversity is driven by variation at the DNA level (Nature) and Environment (Nurture). 
This variation can be partitioned into two major sources. One comes from the mixing of DNA by 
and from our parents. The second major source is the environment and its effect.

The use of computational tools in understanding how Nature and Nurture intertwine is the 
focus of this thesis. Variation observed at the DNA level can be measured using genotyping 
and nextGEN sequencing techniques allowing us to create detailed maps of inheritance. These 
maps allow us to track the origin of DNA. On the other side at the phenotype end of the spectrum 
new high throughput technologies have been developed. Allowing us to perform automated 
phenotyping, metabolomics, proteomics, microarrays and RNAseq to measure all levels from the 
DNA to the observed phenotype at decreasing costs.

How the activity of DNA is transfered and modified by signals from the enviroment is of 
critical importance to our understanding of biology. Variation observed at levels 
'further away' from the genome such as: transcriptome, proteome, metabolome, all the way 
up to classical phenotypes is a complex interplay between DNA and environment. This 
interplay is challenging to unravel because of multiple factors:\\
\begin{itemize}
\item Complex phenotypes are caused by multiple genetic loci, interactions between these, 
modulated by the environment.
\item The more factors underlying a complex phenotype, the larger the samplesize required 
to determine which genetic factors are involved.
\item Data collection however remains limited by resources available (money, sample size, labour, etc)
\item Large scale data collection puts more and more demands on our current infrastructure for 
storage and sharing of this data.
\item Computational issues arise in the analysis of such big data sets (CPU time, RAM requirements, 
Power consumption).
\item Biologie.
\end{itemize}

This thesis will go into these issues, the infrastructure created, methodologies developed, and 
show their application in current research.

\subsection{Background}

Using RLPF markers to build genetic maps - Botstein and Lander (1983)\\
Normal model QTL mapping analogous to ANOVA - Botstein and Lander (1989)\\
Multiple QTL mapping - Jansen et al. (1993)\\
Composite interval mapping -  Zeng et al. (1993)\\
R/qtl (2003)\\
Human genome project 2000-2003\\
First human Genome Wide Association Study (2005)\\
Encyclopedia Of DNA Elements (Encode) (2003, 2007-Current)


\subsection{Thesis outline}

The first chapter: Pheno2Geno deals with the creation of genetic maps from large scale omics data. 
The theory behind genetic map construction is ~100 years old and in a time 50 phenotype markers was 
considered a dense genetic map. In recent years software has been developed to do genetic map 
construction. However looking at the software available for genetic maps construction we observe 
that many packages date from the 1980s and 1990s and have not been adapted yet to use new 
technologies such as multi threading or cluster computing. Pheno2Geno aims to provide a platform to 
deal with this avalance of big data. And aims to provide analysis of data from tilling arrays, 
RNAseq and next generation sequencing to generate expression markers. These markers are used to 
create high density genetic maps. The Pheno2Geno package is writen in the R language for statistical 
computing, and is part of the R/qtl toolset.

The second chapter is the continuation of the Multiple QTL mapping work done by R. C. Jansen, we 
incorporated his QTL mapping method in the R/qtl package. Adding a 'new' algorithm to the R/qtl 
toolset which aims to provide a range of QTL mapping tools based on a single datastructure. 
This allows researchers to analyse data comming from different sources or to change approaches when 
data varies in structure from trait to trait.

In the third chapter we show case our ideas for a generic storage and computation platform. Our demo 
system xQTL workbench is currently being used to run the WormQTL database. xQTL workbench allows 
users to store and share their data, but also run analysis across datasets using the power of 
distributed computing. It comes standard with  QTL mapping tools such as: R/qtl, PLINK and qtlbim

In the last chapter we describe current work on using differences in correlation to generate 
interaction networks. Correlated Traits Locus analysis (or CTL mapping) tries to find genetic 
loci controlling observed correlation differences. A variation on this method has already proven 
valuable in discovering cell type specific eQTL effects. These effects can be used to unmix cell 
mixtures seen in for example whole blood gene expression data. This is usefull for two reasons, 
1) No more costly cell sorting of whole blood 2) no more physical changes to cells (e.g. activation, 
cell death). In short this proposed method creates a proxy for cell counts in an unrelated cohort. 
this proxy is then used during meta analysis as artifical cell count across many different cohorts.
The interaction effects between QTL and Proxy are used to assign a cell type to each eQTL. In the final 
stage diseases in which cell type enrichment is prominent are tested by using publicly available data.\\\\

I hope you enjoy reading this thesis as much as I enjoyed creating it during the last four years,\\\\

Danny Arends (Aug 2013)
